{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21fa2e13-567d-4509-9023-c99fb230f31f",
   "metadata": {},
   "source": [
    "# **4.0 Lets add Memory and Context to our Agent**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "In this tutorial, we’ll build an interactive AI agent using **LangChain**, **LangGraph**, and **Memory**. The agent will:\n",
    "\n",
    "- **Remember previous interactions** for more context-aware responses.\n",
    "- Use **LangGraph** to manage decision-making and visualize workflows.\n",
    "- Leverage **LangChain tools** to interact with network devices and analyze logs.\n",
    "\n",
    "By the end, you’ll have a fully functional AI agent that processes commands, retains context, and helps with network troubleshooting.\n",
    "\n",
    "\n",
    "## **Workshop Outline**\n",
    "\n",
    "1. **Setup**: Install required libraries and configure the environment.\n",
    "2. **LangChain Agent**: Initialize the agent with memory.\n",
    "3. **LangGraph**: Visualize decision-making and task flow.\n",
    "4. **Tool Integration**: Implement network command execution and log analysis.\n",
    "5. **Testing & Visualization**: Test the agent and visualize its decision-making.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a2f515",
   "metadata": {},
   "source": [
    "### **Complete the Following Pre-Requisites**  \n",
    "1. Select the kernel: **\"Python(ai-agent)\"**  \n",
    "2. Perform **Clear all outputs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195d8eba",
   "metadata": {},
   "source": [
    "### Step 1: Warning Control\n",
    "\n",
    "In this step, we import the `warnings` module and suppress any warning messages that may appear during code execution.\n",
    "\n",
    "- **`import warnings`**: Imports the Python `warnings` module to handle warning messages.\n",
    "- **`warnings.filterwarnings('ignore')`**: Instructs Python to ignore all warnings, commonly used to keep the output clean when warnings are not critical.\n",
    "\n",
    "This helps reduce clutter in the output, ensuring that only important results are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5762271-8736-4e94-9444-8c92bd0e8074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b0e3d7",
   "metadata": {},
   "source": [
    "## Step 2: Importing Required Modules and Initializing Memory\n",
    "\n",
    "In this step, we import various modules required for building the agent and initialize in-memory storage for tracking state and memory.\n",
    "\n",
    "- **`from langgraph.graph import StateGraph, END`**: Imports `StateGraph` for managing decision flows and `END` for terminating the flow.\n",
    "- **`from langchain_openai import ChatOpenAI`**: Imports the `ChatOpenAI` model to interact with the language model.\n",
    "- **`from langgraph.checkpoint.sqlite import SqliteSaver`**: Imports `SqliteSaver` for saving the state in SQLite, allowing the agent to maintain memory.\n",
    "- **`from contextlib import ExitStack`**: Used to manage context for resources like memory or databases.\n",
    "- **`from dotenv import load_dotenv`**: Loads environment variables from a `.env` file.\n",
    "\n",
    "The **`ExitStack()`** context manager is used to initialize **in-memory SQLite storage** with **`SqliteSaver`**, which helps in saving and retrieving memory states during the agent's interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0168aee-bce9-4d60-b827-f86a88187e31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from contextlib import ExitStack\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()\n",
    "\n",
    "stack = ExitStack()\n",
    "memory = stack.enter_context(SqliteSaver.from_conn_string(\":memory:\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b002b4b",
   "metadata": {},
   "source": [
    "## Step 3: Initializing the Tool\n",
    "\n",
    "In this step, we initialize a tool for search functionality and print its type and name.\n",
    "\n",
    "- **`tool = TavilySearchResults(max_results=4)`**: Initializes the `TavilySearchResults` tool with a limit of 4 results per query.\n",
    "- **`print(type(tool))`**: Prints the type of the initialized tool to confirm its class.\n",
    "- **`print(tool.name)`**: Prints the name of the tool to confirm its functionality.\n",
    "\n",
    "This step allows us to verify that the tool is set up correctly with the sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589c5b6-6cc2-4594-9a17-dccdcf676054",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = TavilySearchResults(max_results=4) #increased number of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991c235",
   "metadata": {},
   "source": [
    "## Step 4: Defining the Agent's State\n",
    "\n",
    "In this step, we define a class to represent the state of the agent. The state includes the messages that are exchanged during the interaction.\n",
    "\n",
    "- **`class AgentState(TypedDict)`**: Defines a `TypedDict` class named `AgentState`, which is a dictionary-like object that includes the types for the state.\n",
    "- **`messages: Annotated[list[AnyMessage], operator.add]`**: Specifies that the `messages` field will store a list of `AnyMessage` objects, and it uses the `operator.add` function to potentially modify or extend the list of messages.\n",
    "\n",
    "This step helps to structure and manage the agent’s state, including handling the flow of messages throughout the interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba84ec-c172-4de7-ac55-e3158a531b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0675eb9a",
   "metadata": {},
   "source": [
    "## Step 5: Defining the Agent Class\n",
    "\n",
    "We define the `Agent` class to manage the agent’s workflow, interactions, and actions:\n",
    "\n",
    "- **`__init__(self, model, tools, checkpointer, system=\"\")`**: Initializes the agent with a language model, tools, and state graph.\n",
    "  - **`graph.add_node()`**: Adds nodes for interacting with the model and performing actions.\n",
    "  - **`graph.add_conditional_edges()`**: Determines whether to take action or finish.\n",
    "\n",
    "- **`exists_action(self, state)`**: Checks if an action is required based on tool calls.\n",
    "- **`call_openai(self, state)`**: Invokes the language model with current messages.\n",
    "- **`take_action(self, state)`**: Executes actions based on tool calls and returns results.\n",
    "\n",
    "This class manages input, decision-making, and tool interaction.\n",
    "\n",
    "> Note: in `take_action` below, some logic was added to cover the case that the LLM returned a non-existent tool name. Even with function calling, LLMs can still occasionally hallucinate. Note that all that is done is instructing the LLM to try again! An advantage of an agentic organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d5092-b8ef-4e38-b4d7-0e80c609bf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, model, tools, checkpointer, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\",\n",
    "            self.exists_action,\n",
    "            {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            if not t['name'] in self.tools:      # check for bad tool name from LLM\n",
    "                print(\"\\n ....bad tool name....\")\n",
    "                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n",
    "            else:\n",
    "                result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f2fea9",
   "metadata": {},
   "source": [
    "## Step 6: Defining the Agent's Prompt and Initializing the Agent\n",
    "\n",
    "In this step, we define the agent's prompt and initialize the agent with the model and tools.\n",
    "\n",
    "- **`prompt`**: Defines the system message, guiding the agent's behavior for research tasks. It tells the agent when and how to use the search engine for information retrieval.\n",
    "  \n",
    "- **`model = ChatOpenAI(model=\"gpt-3.5-turbo\")`**: Initializes the language model (GPT-3.5).\n",
    "\n",
    "- **`abot = Agent(model, [tool], system=prompt, checkpointer=memory)`**: Initializes the `Agent` with the model, tools, system prompt, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10084a02-2928-4945-9f7c-ad3f5b33caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")  \n",
    "abot = Agent(model, [tool], system=prompt,checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372315ff",
   "metadata": {},
   "source": [
    "## Step 7: Displaying the Agent's Workflow\n",
    "\n",
    "This step adds a directory to the system’s `PATH` and visualizes the agent's workflow graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d6f5f4-2392-41b9-ab96-7919840baa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + \"/opt/homebrew/bin\"\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(abot.graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a11b0e2",
   "metadata": {},
   "source": [
    "## Step 8: Sending Messages to the Agent\n",
    "\n",
    "In this step, we send a message to the agent and process the response.\n",
    "\n",
    "- We define a `messages` list with the user input.\n",
    "- A `thread` is created to maintain the context.\n",
    "- The agent processes the input using the `.stream()` method and the response is printed.\n",
    "\n",
    "This allows real-time interaction with the agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83588e70-254f-4f83-a510-c8ae81e729b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Whats the currency of Lisbon\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a06a8c-fcd4-4ca6-98f0-36c5809813e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What about in India?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb3ef4c-58b3-401b-b104-0d51e553d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Which one is more stronger?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b0ee6b",
   "metadata": {},
   "source": [
    "## Now lets try with a different thread id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3293b7-a50c-43c8-a022-8975e1e444b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Which one is stronger?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08a73a1",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we built an interactive AI agent using **LangChain**, **LangGraph**, and **Memory**. We:\n",
    "\n",
    "- Initialized the agent with a language model and tools.\n",
    "- Implemented memory to track previous interactions.\n",
    "- Used LangGraph to visualize the agent's decision-making process.\n",
    "- Interacted with the agent in real-time, sending messages and processing responses.\n",
    "\n",
    "By the end of this tutorial, you now have a fully functional AI agent that can make decisions, interact with external tools, and retain context across conversations. You can extend this agent with additional tools, improve its decision-making capabilities, and further customize its behavior to fit specific use cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(ai-agent)",
   "language": "python",
   "name": "ai-agent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
