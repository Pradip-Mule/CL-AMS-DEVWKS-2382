{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 RAG over documents for Reliable AI Responses\n",
    "\n",
    "In this workshop, we will walk through the steps of building a retrieval-augmented generation (RAG) agent using LangChain and LangGraph. The agent will be able to answer user queries based on multiple documents (PDFs in this case) using text extraction, chunking, vector storage, and an LLM-based generation process.\n",
    "\n",
    "\n",
    "### Step 1: Importing Required Libraries\n",
    "\n",
    "In this step, we import essential libraries for **PDF text extraction, embeddings, vector storage, and AI-driven processing**:\n",
    "\n",
    "- **`fitz (PyMuPDF)`**: Extracts text from PDFs.  \n",
    "- **`OpenAIEmbeddings` & `Chroma`**: Converts text into embeddings and stores them for retrieval.  \n",
    "- **`RecursiveCharacterTextSplitter`**: Splits text into manageable chunks.  \n",
    "- **`OpenAI`**: Interfaces with OpenAI’s language models for text processing.  \n",
    "- **`StateGraph`**: Manages decision flows in LangGraph-based AI agents.  \n",
    "- **`TypedDict, List`**: Provides structured data handling.  \n",
    "\n",
    "This step prepares the foundation for **processing documents, storing vector embeddings, and enabling AI-driven text analysis** in our agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF for PDF text extraction\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import OpenAI\n",
    "from langgraph.graph import StateGraph\n",
    "from typing import TypedDict, List\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Lets Initialize LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Extract Text from Multiple PDFs\n",
    "In this step, we will create a function to extract text from multiple PDF documents. The function extract_text_from_pdfs is designed to accept a list of PDF file paths as input and then extract the text content from each PDF using the PyMuPDF library (also known as fitz).\n",
    "\n",
    "The function works by:\n",
    "\n",
    "Iterating over each PDF in the list of file paths.\n",
    "Opening the PDF with fitz.open(pdf_path).\n",
    "Extracting the text from each page of the PDF using page.get_text(\"text\").\n",
    "Combining the extracted text from all pages of each document into a single string.\n",
    "Compiling the extracted text from all documents into a list, which is returned as the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Extract text from multiple PDFs\n",
    "def extract_text_from_pdfs(pdf_paths):\n",
    "    \"\"\"Extracts text from multiple PDFs and returns a list of documents.\"\"\"\n",
    "    all_text = []\n",
    "    for pdf_path in pdf_paths:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "        all_text.append(text)\n",
    "    return all_text\n",
    "\n",
    "# Example: List of PDFs to process\n",
    "pdf_files = [\"./data_source/ietf-srv6.pdf\", \"./data_source/SRv6-Mig-BP.pdf\"]  # Add your PDF paths\n",
    "documents_text = extract_text_from_pdfs(pdf_files)\n",
    "\n",
    "# ✅ Split text into smaller chunks for better retrieval\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "all_chunks = []\n",
    "for text in documents_text:\n",
    "    all_chunks.extend(text_splitter.split_text(text))\n",
    "\n",
    "# ✅ Convert chunks into Document objects\n",
    "documents = [Document(page_content=chunk) for chunk in all_chunks]\n",
    "\n",
    "# ✅ Initialize vector database\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define State for LangGraph\n",
    "\n",
    "In this step, we define a **state class** to manage and store data within **LangGraph** for a **Retrieval-Augmented Generation (RAG) process**. This state will track key elements, ensuring the agent has structured information for processing queries and generating responses.\n",
    "\n",
    "#### Why Define a State?\n",
    "LangGraph requires a way to store and update data as the agent moves through different steps. By using Python’s **TypedDict**, we create a structured state with predefined keys and their expected data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Define state for LangGraph\n",
    "class RAGState(TypedDict):\n",
    "    query: str\n",
    "    documents: List[Document]\n",
    "    response: str  # Holds the final answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 LangGraph Workflow for RAG (Retrieval-Augmented Generation)\n",
    "In this step, we create a LangGraph workflow for a Retrieval-Augmented Generation (RAG) system. The workflow involves two key functions: retrieving documents relevant to a query and generating an answer using those documents with an LLM. We also build a stateful graph that connects these two functions and compiles the process into an executable application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ Retrieval function (search across both PDFs)\n",
    "def retrieve_documents(state: RAGState) -> RAGState:\n",
    "    docs = vectorstore.similarity_search(state[\"query\"])\n",
    "    return {\"query\": state[\"query\"], \"documents\": docs, \"response\": \"\"}\n",
    "\n",
    "# ✅ Answer generation function (LLM-based response)\n",
    "def generate_answer(state: RAGState) -> RAGState:\n",
    "    context = \"\\n\".join([doc.page_content for doc in state[\"documents\"]])\n",
    "    prompt = f\"Based on the following context, answer the question:\\n\\n{context}\\n\\nQuestion: {state['query']}\"\n",
    "    answer = llm.invoke(prompt)\n",
    "    return {\"query\": state[\"query\"], \"documents\": state[\"documents\"], \"response\": answer}\n",
    "\n",
    "# ✅ Build LangGraph\n",
    "graph = StateGraph(RAGState)\n",
    "graph.add_node(\"retrieval\", retrieve_documents)\n",
    "graph.add_node(\"generation\", generate_answer)\n",
    "graph.add_edge(\"retrieval\", \"generation\")\n",
    "\n",
    "# ✅ Define entry point\n",
    "graph.set_entry_point(\"retrieval\")\n",
    "\n",
    "# ✅ Compile the graph\n",
    "app = graph.compile()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Defining the Prompt and Query Handling\n",
    "\n",
    "In this step, we define a **prompt** to guide the agent in answering queries related to **SRv6, micro-SID, and SRv6 Migrations from SR-MPLS**. The prompt ensures the agent provides responses based on the provided documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "Answer: The SRv6 microSID is a 128-bit value used for routing to a specific node responsible for performing a specific function in an SRv6 network. It is represented as an IPv6 address and consists of three parts: the locator, uSID block, and set ID and node ID. The uSID block is the portion of the SRv6 microSID that is used for identifying the specific node responsible for performing the function. It is allocated from a block specifically designated for service plane addresses in an SRv6 network.\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt for the agent\n",
    "agent_prompt = \"You are an expert on SRv6, its micro-sid and SRv6 Migrations from SR-MPLS. Please answer the query based on the provided documents.\"\n",
    "\n",
    "# Define the user query as a variable\n",
    "user_query = input(\"What would you like to know about SRv6, SRv6-uSID or SRv6 Migration Best practices?\")\n",
    "\n",
    "# Invoke the agent with the prompt and the user query\n",
    "response = app.invoke({\n",
    "    \"query\": user_query,\n",
    "    \"documents\": [],  # Include documents here if applicable\n",
    "    \"response\": \"\",\n",
    "    \"prompt\": agent_prompt  # Adding the prompt to guide the agent's response\n",
    "})\n",
    "\n",
    "# Print the response from the agent\n",
    "print(response[\"response\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(ai-agent)",
   "language": "python",
   "name": "ai-agent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
