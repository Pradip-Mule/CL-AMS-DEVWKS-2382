{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Interactive AI Agents for Information Retrieval\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "In this tutorial, we will:\n",
    "\n",
    "- **Create an interactive AI agent** capable of searching for information based on user queries.\n",
    "- **Leverage LangGraph** to manage the decision-making process, handling multiple tool calls and responses.\n",
    "- **Visualize the workflow** of our AI agent to better understand its operations.\n",
    "\n",
    "\n",
    "## **Workshop Outline**\n",
    "\n",
    "1. **Setup**: Install libraries and prepare the environment.\n",
    "2. **Agent Creation**: Design the agent and configure LangGraph.\n",
    "3. **Tool Integration**: Implement search tools and handle responses.\n",
    "4. **Workflow**: Manage the agent’s decision flow and responses.\n",
    "5. **Visualization**: Visualize the decision-making process with LangGraph.\n",
    "6. **Testing**: Interact with the agent through queries.\n",
    "7. **Next Steps**: Enhance the agent and explore use cases.\n",
    "\n",
    "\n",
    "By the end of this workshop, you’ll have hands-on experience in building interactive, intelligent agents capable of managing multiple tools and making decisions based on real-time queries!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Complete the Following Pre-Requisites**  \n",
    "1. Select the kernel: **\"Python(ai-agent)\"**  \n",
    "2. Perform **Clear all outputs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Warning Control\n",
    "\n",
    "In this cell, we are importing the `warnings` module and suppressing any warning messages that may appear during the execution of the code.  \n",
    "\n",
    "- **`import warnings`**: This imports the Python `warnings` module, which is used to handle warning messages that are generated during code execution.\n",
    "- **`warnings.filterwarnings('ignore')`**: This command tells Python to ignore all warnings. It's commonly used in notebooks to keep the output clean, especially when you know that certain warnings are not critical to the execution or the purpose of the notebook.\n",
    "\n",
    "This step is helpful for reducing unnecessary clutter in the output, ensuring that only the important results are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Importing Libraries and Loading Environment Variables\n",
    "\n",
    "We import essential libraries for building the AI agent:\n",
    "\n",
    "- **`load_dotenv`**: Loads environment variables from a `.env` file.\n",
    "- **`StateGraph` and `END` from LangGraph**: Manages state transitions and ends the process.\n",
    "- **Type annotations**: `TypedDict` and `Annotated` help define structured data types.\n",
    "- **Operator**: Used for basic operations like comparisons.\n",
    "- **LangChain message types**: Handles different message types like `SystemMessage`, `HumanMessage`, etc.\n",
    "- **`ChatOpenAI`**: Integrates GPT models for chat-based interactions.\n",
    "- **`TavilySearchResults`**: Fetches search results from Tavily.\n",
    "\n",
    "These libraries are key for managing the agent's interactions, state, and external searches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Initializing the Tools\n",
    "\n",
    "In this step, we initialize the **TavilySearchResults** tool, which will allow our agent to retrieve information during its execution. \n",
    "This tool helps in fetching search results from Tavily, enabling the agent to gather relevant data for user queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the search tool with more results for a comprehensive answer\n",
    "tool = TavilySearchResults(max_results=4) \n",
    "print(type(tool))  # Checking the tool type\n",
    "print(tool.name)   # Displaying the tool's name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Defining the Agent State and Behavior\n",
    "\n",
    "In this step, we define the agent's state and behavior. Using **LangGraph**, the agent will manage the flow between the decision-making process (handled by the language model, LLM) and the actions (such as retrieving information via the search tool). This ensures that the agent can intelligently process messages, make decisions, and perform actions accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the agent's state with messages\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "\n",
    "# Define the Agent class\n",
    "class Agent:\n",
    "    def __init__(self, model, tools, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)  # Create a new state graph for the agent\n",
    "        graph.add_node(\"llm\", self.call_openai)  # Add LLM node for interacting with OpenAI\n",
    "        graph.add_node(\"action\", self.take_action)  # Add action node for interacting with tools\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\", \n",
    "            self.exists_action, \n",
    "            {True: \"action\", False: END}  # If action exists, move to 'action' node, else end\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")  # Return to LLM after action\n",
    "        graph.set_entry_point(\"llm\")  # Set entry point to LLM\n",
    "        self.graph = graph.compile()\n",
    "        self.tools = {t.name: t for t in tools}  # Map tool names to tool instances\n",
    "        self.model = model.bind_tools(tools)  # Bind tools to the model\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0  # Check if tool calls are present\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages  # Add system message if provided\n",
    "        message = self.model.invoke(messages)  # Invoke model to get a response\n",
    "        return {'messages': [message]}  # Return response as a list of messages\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls  # Get tool calls from the last message\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            if not t['name'] in self.tools:  # Check for valid tool name\n",
    "                print(\"\\n ....bad tool name....\")\n",
    "                result = \"bad tool name, retry\"  # Retry if tool name is invalid\n",
    "            else:\n",
    "                result = self.tools[t['name']].invoke(t['args'])  # Invoke the tool with arguments\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))  # Store the result\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}  # Return the results to the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Defining the System Prompt and Initializing the Agent\n",
    "\n",
    "In this step, we define a **system prompt** to guide the agent's behavior and initialize it with a language model and search tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the agent\n",
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\"\"\"\n",
    "\n",
    "# Initialize the model and agent\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")  # Using a lower cost model\n",
    "abot = Agent(model, [tool], system=prompt)  # Initialize the agent with the model and tool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Visualizing the Agent's Decision Flow\n",
    "\n",
    "In this step, we use **LangGraph** to visualize the agent's decision flow. This visualization helps us understand how the agent transitions between nodes, making decisions and taking actions based on user inputs and the available tools. By viewing the workflow, we gain insight into the agent's process, improving transparency and debugging capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the agent's decision flow\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + \"/opt/homebrew/bin\"\n",
    "from IPython.display import Image\n",
    "Image(abot.graph.get_graph().draw_png())  # Displaying the graph as a PNG image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Interacting with the Agent\n",
    "\n",
    "In this step, we interact with the agent by providing user inputs. Based on the input, the agent will process the query and, if necessary, invoke the search tool to gather additional information. This allows the agent to dynamically respond to user queries and take actions accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent with a simple user input\n",
    "messages = [HumanMessage(content=\"who won the latest nobel prize in physics?\")]\n",
    "result = abot.graph.invoke({\"messages\": messages})\n",
    "print(result['messages'][-1].content)  # Output the result of the agent's response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a more complex query\n",
    "messages = [HumanMessage(content=\"Who won the 2024 nobel prize in physics? and when is his/her Birthday?\")]\n",
    "result = abot.graph.invoke({\"messages\": messages})\n",
    "print(result['messages'][-1].content)  # Output the result of the agent's response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Advanced Query Handling\n",
    "\n",
    "In this step, we test the agent with a more complex query. The agent will make multiple tool calls, processing each one sequentially, to gather the necessary information. After collecting the data, the agent will provide a more detailed and comprehensive answer based on the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with an advanced query\n",
    "query = \"Who won the Super Bowl in 2024? In what state is the winning team headquartered? \\\n",
    "What is the GDP of that state? Answer each question.\"\n",
    "messages = [HumanMessage(content=query)]\n",
    "model = ChatOpenAI(model=\"gpt-4o\")  # Using a more advanced model for complex queries\n",
    "abot = Agent(model, [tool], system=prompt)\n",
    "result = abot.graph.invoke({\"messages\": messages})\n",
    "\n",
    "# Output the result\n",
    "print(result['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Now let's run a UI for our Interactive AI Agent with Langgraph.\n",
    "\n",
    "Go to the terminal and lets run this example in the browser for you to interact with it seemlessly.\n",
    "\n",
    "1. open a new Terminal and then choose command prompt(cmd)\n",
    "2. run this command >streamlit run C:\\Users\\admin\\Desktop\\devwks\\DEVWKS-2382\\UI\\Interactice-AI-Agent-with-langgraph_UI.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You've built an interactive AI agent using LangGraph.\n",
    "\n",
    "### Key Takeaways:\n",
    "- **LangGraph** offers a structured framework for creating decision-making AI agents.\n",
    "- **External tools**, like search engines, can be seamlessly integrated into the agent’s workflow for advanced capabilities.\n",
    "- **Visualization** allows you to see and understand the agent's decision-making process, improving transparency.\n",
    "\n",
    "Think about the many possibilities for using LangGraph in automating tasks, enhancing troubleshooting, or building even more intelligent and dynamic agents in the future!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(ai-agent)",
   "language": "python",
   "name": "ai-agent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
